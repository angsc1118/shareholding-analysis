# 2024-12-16 22:20:00: [Feat] æ­·å²é‡è¼‰å·¥å…· (å¾ Storage æ¢å¾©è³‡æ–™)
import os
import sys
import argparse
from datetime import datetime
from supabase import create_client, Client
from utils import clean_and_transform_data  # é‡ç”¨æ¸…æ´—é‚è¼¯

# --- è¨­å®š ---
SUPABASE_URL = (os.environ.get("SUPABASE_URL") or "").strip().rstrip("/")
SUPABASE_KEY = (os.environ.get("SUPABASE_SERVICE_KEY") or "").strip()
BUCKET_NAME = "tdcc_raw_files"

if not SUPABASE_URL or not SUPABASE_KEY:
    print("âŒ éŒ¯èª¤: ç¼ºå°‘ç’°å¢ƒè®Šæ•¸")
    sys.exit(1)

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

def process_single_file(file_name):
    """ä¸‹è¼‰å–®ä¸€æª”æ¡ˆä¸¦è™•ç†å¯«å…¥"""
    print(f"\nğŸ“‚ æ­£åœ¨è™•ç†æª”æ¡ˆ: {file_name}")
    
    # 1. ä¸‹è¼‰
    try:
        print("   â¬‡ï¸  æ­£åœ¨ä¸‹è¼‰ Bytes...")
        data = supabase.storage.from_(BUCKET_NAME).download(file_name)
    except Exception as e:
        print(f"   âŒ ä¸‹è¼‰å¤±æ•— (æª”æ¡ˆæ˜¯å¦å­˜åœ¨?): {e}")
        return

    # 2. æ¸…æ´—
    try:
        print("   ğŸ§¹ æ­£åœ¨æ¸…æ´— (å¥—ç”¨æœ€æ–°è¦å‰‡)...")
        df = clean_and_transform_data(data)
        print(f"   âœ… æ¸…æ´—å®Œæˆ: {len(df)} ç­†æœ‰æ•ˆè³‡æ–™")
    except Exception as e:
        print(f"   âŒ æ¸…æ´—å¤±æ•—: {e}")
        return

    # 3. å¯«å…¥
    print("   ğŸ“¤ æ­£åœ¨å¯«å…¥è³‡æ–™åº«...")
    records = df.to_dict(orient='records')
    BATCH_SIZE = 1000
    try:
        for i in range(0, len(records), BATCH_SIZE):
            batch = records[i : i + BATCH_SIZE]
            supabase.table("equity_distribution").upsert(batch).execute()
        print("   âœ… å¯«å…¥æˆåŠŸï¼")
    except Exception as e:
        print(f"   âŒ å¯«å…¥å¤±æ•—: {e}")

def list_and_process_all():
    """åˆ—å‡º Bucket æ‰€æœ‰æª”æ¡ˆä¸¦ä¾åºè™•ç†"""
    print("ğŸ” æ­£åœ¨åˆ—å‡º Storage æ‰€æœ‰æª”æ¡ˆ...")
    try:
        # æ³¨æ„: list æ–¹æ³•é è¨­æœ‰åˆ†é é™åˆ¶ (é€šå¸¸ 100 ç­†)ï¼Œè‹¥æª”æ¡ˆæ¥µå¤šéœ€æ”¹å¯«ç‚ºè¿´åœˆåˆ†é 
        files = supabase.storage.from_(BUCKET_NAME).list()
        
        # éæ¿¾å‡º CSV æª”
        csv_files = [f['name'] for f in files if f['name'].endswith('.csv')]
        
        if not csv_files:
            print("âš ï¸  æ‰¾ä¸åˆ°ä»»ä½• CSV æª”æ¡ˆã€‚")
            return

        print(f"ğŸ“‹ æ‰¾åˆ° {len(csv_files)} å€‹æª”æ¡ˆï¼Œæº–å‚™é–‹å§‹å›è£œ...")
        
        # æ’åºï¼Œå¾èˆŠåˆ°æ–°åŸ·è¡Œ
        csv_files.sort()
        
        for fname in csv_files:
            process_single_file(fname)
            
    except Exception as e:
        print(f"âŒ åˆ—å‡ºæª”æ¡ˆå¤±æ•—: {e}")

if __name__ == "__main__":
    # è¨­å®šæŒ‡ä»¤åƒæ•¸
    parser = argparse.ArgumentParser(description='TDCC æ­·å²è³‡æ–™é‡è¼‰å·¥å…·')
    parser.add_argument('--file', type=str, help='æŒ‡å®šç‰¹å®šæª”åé‡è·‘ (ä¾‹å¦‚: TDCC_20251216.csv)')
    parser.add_argument('--all', action='store_true', help='é‡è·‘ Storage å…§æ‰€æœ‰æª”æ¡ˆ')
    
    args = parser.parse_args()

    print(f"ğŸ› ï¸  å•Ÿå‹•æ­·å²é‡è¼‰å·¥å…·: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    if args.file:
        process_single_file(args.file)
    elif args.all:
        list_and_process_all()
    else:
        print("âš ï¸  è«‹æŒ‡å®šåƒæ•¸: --file [æª”å] æˆ– --all")
        print("   ç¯„ä¾‹: python src/reload_history.py --file TDCC_20251216.csv")
