# 2024-12-16 19:00:00: [Feat] å®Œæ•´ ETL è…³æœ¬ï¼šä¸‹è¼‰ã€å‚™ä»½ã€æ¸…æ´—ã€å…¥åº«
import os
import sys
import requests
import pandas as pd
import io
from datetime import datetime
from supabase import create_client, Client

# --- 1. è¨­å®šèˆ‡å¸¸æ•¸ ---
TDCC_URL = "https://smart.tdcc.com.tw/opendata/getOD.ashx?id=1-5"
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_SERVICE_KEY")
BUCKET_NAME = "tdcc_raw_files"

# æª¢æŸ¥ç’°å¢ƒè®Šæ•¸
if not SUPABASE_URL or not SUPABASE_KEY:
    print("âŒ éŒ¯èª¤: ç¼ºå°‘ç’°å¢ƒè®Šæ•¸ SUPABASE_URL æˆ– SUPABASE_SERVICE_KEY")
    sys.exit(1)

# åˆå§‹åŒ– Supabase
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

def run_etl():
    print(f"ğŸš€ é–‹å§‹åŸ·è¡Œ ETL ä»»å‹™: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    # --- 2. ä¸‹è¼‰è³‡æ–™ (Extract) ---
    print("ğŸ“¥ æ­£åœ¨å¾é›†ä¿ä¸­å¿ƒä¸‹è¼‰ CSV...")
    try:
        response = requests.get(TDCC_URL, timeout=30)
        response.raise_for_status()
        raw_content = response.content
        print(f"   ä¸‹è¼‰æˆåŠŸï¼æª”æ¡ˆå¤§å°: {len(raw_content) / 1024:.2f} KB")
    except Exception as e:
        print(f"âŒ ä¸‹è¼‰å¤±æ•—: {e}")
        sys.exit(1)

    # --- 3. å‚™ä»½åŸå§‹æª” (Backup to Storage) ---
    # æª”åæ ¼å¼: TDCC_YYYYMMDD.csv
    today_str = datetime.now().strftime("%Y%m%d")
    backup_filename = f"TDCC_{today_str}.csv"
    
    print(f"ğŸ’¾ æ­£åœ¨å‚™ä»½åŸå§‹æª”è‡³ Storage: {backup_filename}...")
    try:
        supabase.storage.from_(BUCKET_NAME).upload(
            path=backup_filename,
            file=raw_content,
            file_options={"content-type": "text/csv", "upsert": "true"}
        )
        print("   âœ… å‚™ä»½æˆåŠŸï¼")
    except Exception as e:
        print(f"âš ï¸ å‚™ä»½å¤±æ•— (å¯èƒ½æ˜¯æª”æ¡ˆå·²å­˜åœ¨æˆ–ç¶²è·¯å•é¡Œ): {e}")
        # æ³¨æ„ï¼šå‚™ä»½å¤±æ•—ä¸æ‡‰é˜»æ“‹å¾ŒçºŒæ¸…æ´—ï¼Œé™¤éæ‚¨å¸Œæœ›åš´æ ¼åŸ·è¡Œ
    
    # --- 4. è³‡æ–™æ¸…æ´— (Transform) ---
    print("ğŸ§¹ æ­£åœ¨æ¸…æ´—è³‡æ–™...")
    try:
        # ä½¿ç”¨ Big5 è§£ç¢¼è®€å– CSV
        # header=0 è¡¨ç¤ºç¬¬ä¸€åˆ—æ˜¯æ¨™é¡Œ
        df = pd.read_csv(io.BytesIO(raw_content), encoding="big5", dtype=str)
        
        # é‡æ–°å‘½åæ¬„ä½ (å°æ‡‰è³‡æ–™åº«æ¬„ä½)
        # åŸå§‹æ¬„ä½é€šå¸¸æ˜¯: è³‡æ–™æ—¥æœŸ,è­‰åˆ¸ä»£è™Ÿ,æŒè‚¡åˆ†ç´š,äººæ•¸,è‚¡æ•¸,å é›†ä¿åº«å­˜æ•¸æ¯”ä¾‹%
        df.columns = ["date", "stock_id", "level", "persons", "shares", "percent"]

        # è³‡æ–™è½‰æ›é‚è¼¯
        # 1. æ°‘åœ‹å¹´è½‰è¥¿å…ƒå¹´ (ä¾‹å¦‚ 1120101 -> 2023-01-01)
        def convert_date(roc_date):
            if pd.isna(roc_date): return None
            roc_date = str(roc_date)
            year = int(roc_date[:-4]) + 1911
            month = roc_date[-4:-2]
            day = roc_date[-2:]
            return f"{year}-{month}-{day}"

        df['date'] = df['date'].apply(convert_date)

        # 2. æ•¸å€¼æ¸…æ´— (ç§»é™¤é€—è™Ÿä¸¦è½‰å‹)
        # ç§»é™¤ 'persons', 'shares', 'percent' ä¸­çš„é€—è™Ÿ
        cols_to_clean = ['persons', 'shares', 'percent']
        for col in cols_to_clean:
            df[col] = df[col].str.replace(',', '', regex=False)
            # è½‰ç‚ºæ•¸å­—ï¼Œç„¡æ³•è½‰æ›è®Šæˆ NaN
            df[col] = pd.to_numeric(df[col], errors='coerce')

        # 3. è™•ç† Level (ç§»é™¤èªªæ˜æ–‡å­—ï¼Œåªç•™æ•¸å­—)
        # æœ‰æ™‚å€™ level æœƒæ˜¯ "15 (1000å¼µä»¥ä¸Š)" é€™ç¨®æ ¼å¼ï¼Œéœ€ç¢ºä¿æ˜¯ç´”æ•¸å­—
        # å‡è¨­åŸå§‹è³‡æ–™å·²ç¶“æ˜¯æ•¸å­—åˆ†é¡ 1-17ï¼Œè‹¥ä¸æ˜¯éœ€é¡å¤–è™•ç†ï¼Œé€™è£¡å‡è¨­æ˜¯ç´”æ•¸å­—
        df['level'] = pd.to_numeric(df['level'], errors='coerce')

        # 4. ç§»é™¤å«æœ‰ç©ºå€¼çš„åˆ— (ç¢ºä¿è³‡æ–™å®Œæ•´æ€§)
        df.dropna(subset=['date', 'stock_id', 'level'], inplace=True)

        print(f"   æ¸…æ´—å®Œæˆï¼æº–å‚™å¯«å…¥ {len(df)} ç­†è³‡æ–™...")

    except Exception as e:
        print(f"âŒ è³‡æ–™æ¸…æ´—å¤±æ•—: {e}")
        # å°å‡ºå‰å¹¾è¡Œå¹«åŠ©é™¤éŒ¯
        print("CSV Content Head:", raw_content[:200].decode('big5', errors='ignore'))
        sys.exit(1)

    # --- 5. å¯«å…¥è³‡æ–™åº« (Load / Upsert) ---
    print("ğŸ“¤ æ­£åœ¨å¯«å…¥ Supabase è³‡æ–™åº« (åˆ†æ‰¹å¯«å…¥)...")
    
    # å°‡ DataFrame è½‰ç‚ºå­—å…¸åˆ—è¡¨
    records = df.to_dict(orient='records')
    
    # åˆ†æ‰¹å¯«å…¥ (Batch Insert)ï¼Œé¿å…ä¸€æ¬¡é€å¤ªå¤§å°åŒ…å°è‡´ Timeout
    BATCH_SIZE = 1000
    total_inserted = 0
    
    try:
        for i in range(0, len(records), BATCH_SIZE):
            batch = records[i : i + BATCH_SIZE]
            
            # ä½¿ç”¨ Upsert: å¦‚æœ (date, stock_id, level) è¡çªå‰‡æ›´æ–°
            supabase.table("equity_distribution").upsert(batch).execute()
            
            total_inserted += len(batch)
            print(f"   å·²å¯«å…¥: {total_inserted} / {len(records)}")
            
        print("âœ… ETL ä»»å‹™å…¨éƒ¨å®Œæˆï¼")

    except Exception as e:
        print(f"âŒ è³‡æ–™åº«å¯«å…¥å¤±æ•—: {e}")
        sys.exit(1)

if __name__ == "__main__":
    run_etl()
